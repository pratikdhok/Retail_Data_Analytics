1)Using Function (keep this function, very useful if you get chance to work on JSON data in real time)
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
import res
spark = SparkSession.builder.master("local[*]").appName("test").getOrCreate()
data="mention s3 raw bucket path here"
df=spark.read.format("json").load(data)

def read_nested_json(df):
    column_list = []
    for column_name in df.schema.names:
        if isinstance(df.schema[column_name].dataType, ArrayType):
            df = df.withColumn(column_name, explode(column_name).alias(column_name))
            column_list.append(column_name)
        elif isinstance(df.schema[column_name].dataType, StructType):
            for field in df.schema[column_name].dataType.fields:
                column_list.append(col(column_name + "." + field.name).alias(column_name + "_" + field.name))
        else:
            column_list.append(column_name)
    df = df.select(column_list)
    return df


is_all_columns_flattened = False
while not is_all_columns_flattened:
    # existing columns are flattened and appended to the schema columns
    df = read_nested_json(df)
    is_all_columns_flattened = True
    # check till all new columns are flattened
    for column_name in df.schema.names:
        if isinstance(df.schema[column_name].dataType, ArrayType):
            is_all_columns_flattened = False
        elif isinstance(df.schema[column_name].dataType, StructType):
            is_all_columns_flattened = False


# Function to cast columns to specific data types
def cast_column(df, column_name, data_type):
    return df.withColumn(column_name, df[column_name].cast(data_type))

# Change data types using the defined function
df = cast_column(df, "sales_year", "int")
df = cast_column(df, "sales_month", "int")
df = cast_column(df, "total_revenue", FloatType())

df = df.dropDuplicates(["product_id", "sales_year"])
df = df.withColumnRenamed("total_revenue", "revenue")
df = df.drop("sales_month")

# Sample transformations (Note: in another_df we are storing static file(or lookup file) 
#data which company provides you to join with incoming data)

another_df = spark.read.csv("path/lookup.csv", header=True)
transformed_df = df.join(another_df, on="product_id", how="inner")

#Apply transformations and create a new column using a CASE statement
transformed_df = transformed_df.withColumn("product_category1", 
                   when(col("category") == "electronics", "Electronics")
                   .when(col("category") == "clothing", "Clothing")
                   .when(col("category") == "home", "Home")
                   .otherwise("Other"))

# Filter the DataFrame to include only specific product categories
filtered_df = transformed_df.filter((col("product_category") == "Electronics") | (col("product_category") == "Clothing"))


#  String Manipulation
filtered_df = filtered_df.withColumn("product_name_upper", upper(col("product_name")))

#  Handling Null Values
filtered_df = filtered_df.fillna({"revenue_per_customer": 0})


#Filtering and Ordering
filtered_ordered_df = filtered_df.filter(col("sales_year") == 2023).orderBy(col("total_revenue").desc())

# Define S3 landing bucket path
landing_bucket_path = "s3://your-landing-bucket-path/"

# Write the filtered DataFrame to S3 landing bucket in CSV format
filtered_ordered_df.write.mode('append').format('csv').option('header', 'true').save(landing_bucket_path)





Note:::::
Above transformations are the possible transformations...
number of transformations depends on requirement you get from your manager or TL
But similar type of transformations you need to perform




2)Sample code without using function
Note: for struct column use parent_column.child_column logic
      for array column use explode function to flatten this array column to struct

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
import re
spark = SparkSession.builder.master("local[*]").appName("test").getOrCreate()
data="s3 raw bucket path"
df=spark.read.format("json").load(data)
#df.printSchema()
res=df.withColumn('theme1name',col('theme1.Name')).withColumn('theme1percent',col('theme1.Percent'))\
    .withColumn('theme_namecode',explode(col('theme_namecode'))).\
    withColumn('theme_code',col('theme_namecode.code'))\
    .withColumn('theme_name',col('theme_namecode.name')).drop('theme_namecode','theme1')
final=res.select('theme1name','theme1percent','theme_name')
output_path="s3 landing bucket path"
final.write.mode('overwrite').format('csv').option('header','false').save(output_path)




































